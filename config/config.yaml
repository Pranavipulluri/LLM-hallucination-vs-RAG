model:
  name: "gpt-3.5-turbo"
  temperature: 0.7
  max_tokens: 256

embedding:
  model: "all-mpnet-base-v2"
  batch_size: 64

retrieval:
  top_k: 5
  index_path: "data/processed/wiki.index"
  passages_path: "data/processed/passages.jsonl"

data:
  raw_dir: "data/raw"
  processed_dir: "data/processed"
  train_split: 0.7
  val_split: 0.15
  test_split: 0.15

training:
  lora:
    r: 8
    lora_alpha: 32
    lora_dropout: 0.05
    target_modules: ["q_proj", "v_proj"]
    learning_rate: 2e-4
    epochs: 3
    batch_size: 4
  
  detector:
    model: "roberta-base"
    learning_rate: 2e-5
    epochs: 5
    batch_size: 16

evaluation:
  sample_size: 200
  human_eval_size: 100
